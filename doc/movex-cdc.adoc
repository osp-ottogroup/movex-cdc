= image:osp.png[float="left" width=200 ] MOVEX Change Data Capture  =
Author: Peter Ramm ( Peter.Ramm@ottogroup.com )
:Author Initials: PR
:toc:
:toclevels: 4
:icons:
:imagesdir: ./images
:numbered:
:sectnumlevels: 6
:homepage: https://www.osp.de
:title-logo-image: osp.png
:description: Solution for change data capture from Oracle to Kafka
:keywords: Oracle, Kafka, Change Data Capture, CDC, Trigger

TIP: This page is not yet fully completed and still under construction.

== Introduction ==

This documentation describes the product MOVEX Change Data Capture which is hosted at https://gitlab.com/otto-group-solution-provider/movex-cdc.

=== Purpose ===
**********************************************************************
Captures data change events (Insert/Update/Delete) in Oracle and other databases and immediately transfers the data changes to Kafka event hub.
**********************************************************************


MOVEX CDC allows a system-wide identical and redundancy-free capture of change events on database tables. +
The Kafka-Cluster , which is supplied with events by MOVEX CDC, is responsible for the provision and distribution of events to any consumers (Publish & Subscribe).

=== Concept ===
**********************************************************************
* Database triggers are used to capture change events.
* The configuration for the tables and columns to observe by triggers is stored local in the database in MOVEX CDC's schema.
* This configuration can be maintained manually by the application's web-GUI but can also be loaded as JSON file
(configuration as code in revision control). +
* The database triggers are generated based on this configuration data via web-GUI or API call.
**********************************************************************

Synchronous processing and storage of the trigger events is initially performed locally in the database, without further dependencies on external systems.
The further transmission of the events to Kafka is asynchronous to the trigger processing.

image:event_flow.svg[format=svg,opts=inline]

The focus is on resource-conserving yet stable and high-performance processing,
low complexity in the operation of the solution and minimal intervention in the operation of the database.
In particular, compared with alternative solutions such as Oracle Golden Gate, Quest Shareplex or Red Hat Debezium,
it is not necessary to drastically increase the retention period of the DB online transaction log.

==== Using Kafka keys to ensure sequential order of messages ====
For Kafka consumers the original sequence of messages is guaranteed only for messages consumed from the same partition of a topic. +
Therefore you must place messages within the same partition of a topic if you want to consume them in original order. +
Kafka has the concept of message keys for that. Kafka ensures that messages with the same key value are placed in the same partition and this way are consumed in original sequence.

MOVEX CDC supports four kinds of message keys for Kafka that can be defined by GUI at table level:

* No message key: Messages are placed randomly in partitions
* Fixed value: All change events of a table are placed in the same partition (makes sense only if events of multiple tables are produced to the same Kafka topic)
* Primary key values: Ensures that the change history of a single DB record is always consumed in original sequence
* Transaction-ID: Ensures that all events of a particular DB transaction can be consumed in original sequence


=== Differentiation from other solutions for CDC ===
There are a number of existing solutions for change capture, commercial as well as open source.
Most of them are based on processing of DB's transaction log. +
Using transaction log for CDC ensures that no additional effort is loaded on the primary transactions,
so processing the change events is completely asynchroneous. +
But this solutions also mean:

* Covering outages of CDC target (Kafka) requires later processing of transaction log when CDC target systems become available again
* Therefore you have to preserve the transaction log in space for the longest expected outage of the CDC target, if you expect to continue processing automatically after CDC target system outage
* Including weekend, public holidays and some time for troubleshooting this regularly requires to preserve the DB transaction log in place for at least three days
* Especially for Oracle you have to activate SUPPLEMENTAL LOGGING which significantly increases transaction log sizes
* If you only need a small amount of change events from large transaction processing systems then the effort in dealing with transaction logs becomes complex and expensive compared to what you actually want.

This is the case where MOVEX CDC comes into play. +
Accepting the synchroneous overhead of triggers in business transactions the solution is sized for the expected amount of observed change events independent from the total transaction throughput of the entire database.

.Other common existing solutions for change data capturing and transfer to Kafka
[cols="~,~"]
|===
|Product|Info

|https://debezium.io[Debezium]|Open source solution for several database systems. +
Works with https://docs.oracle.com/database/121/XSTRM/xstrm_intro.htm#XSTRM1086[XStream API] (requires Golden Gate license for consumer) or directly by LogMiner for Oracle.
|https://docs.oracle.com/goldengate/c1230/gg-winux/index.html[Oracle Golden Gate]|
Commercial solution, requires licensing of producer and consumer
|https://www.quest.com/documents/shareplex-for-kafka-target-datasheet-144821.pdf[Quest SharePlex]|
Commercial solution, processes redo log files.
|https://docs.confluent.io/kafka-connect-oracle-cdc/current/index.html[Oracle CDC Source Connector for Confluent Platform:]|
Commercial solution, based on Logminer function.
Not yet functioning for Oracle 19c.
Requires supplemental logging in Oracle DB.
|===

=== Supported databases ===

==== Oracle Database ====
Oracle Databases are supported for release 12.1. and higher.

===== Enterprise or Express Edition with Partitioning Option =====
MOVEX CDC works best if Partitioning Option is available for your database in Enterprise or Express Edition.
Interval partitioning of table Event_Logs is used in this case which ensures automatic shrinking to minimum needed storage footprint. +

===== Standard Edition or Enterprise Edition without Partitioning Option =====
MOVEX CDC also works without partitioning,
but in this case there are some disadvantages:

- Peak usage increases high water mark in table Event_Logs, the claimed space is not freed after processing
- Because read access with full table scan is not suitable in this case due to the unpredictable size of the table, an index on column ID is placed for the non-partitioned table Event_Logs
- This index ensures processing troughput, but a tiny risk is remaining for wait szenarios at index block split operation under heavy concurrent transactions that are executing MOVEX CDC's triggers.


==== SQLite ====
SQLite is used as development database for MOVEX CDC. There might be no useful production use case but it works.

==== PostgreSQL, Microsoft SQL Server ====
Support for PostgreSQL and MS SQL Server is planned in the future. +
The implementation depends on achievable benefits in application and operation compared to simply using the existing open source log-based solution https://debezium.io[Debezium].

== Operation ==
=== Preconditions for usage ===
==== Sizing of server instance for MOVEX Change Data Cature ====
The application runs on one CPU and 4 GB of memory with it's default settings.
But for higher number of worker threads and/or larger memory buffer size you should increase the number or CPUs and memory according. +
By default MOVEX CDC uses up to 75% of the available memory.
If you want to limit the maximum memory used by MOVEC CDC then set JAVA_OPTS=-Xmx to the desired value (like JAVA_OPTS=-Xmx4096m for 4 GB ).


==== Database schema for MOVEX Change Data Cature  ====
The application needs it's own database schema at the observed database. +
This schema contains configuration tables which the application will create itself at first startup as well as the buffered (not yet transferred) events. +
Storage quotas for this schema should allow storage of buffered events as long as the longest possibly expected outage of Kafka that should be covered without restrictions to the business transactions.

==== Rights for the owner of MOVEX CDC's DB schema ====
The owner of the schema requires some preconditions/grants at the database as well as quota on its default tablespace.
The existence of this grants is checked at application start.

To ensure sufficient user rights the schema owner for MOVEX CDC can also be created by the application itself with given DB admin credentials.

===== Rights for the owner of MOVEX CDC's DB schema: Oracle =====

.Minimum grants required to operate MOVEX CDC with Oracle DB
[cols="~,~"]
|===
|Grant|Description

|CONNECT|Allows establishing session
|CREATE ANY TRIGGER|Allows creation and dropping of triggers in foreign schemas of database
|CREATE VIEW|Allows creation of views in MOVEX CDC's DB schema
|RESOURCE|Allows creation of tables in own schema
|SELECT ON sys.DBA_Constraints|For primary key info of table.
|SELECT ON sys.DBA_Cons_Columns|For primary key info of table.
|SELECT ON sys.DBA_Role_Privs|Allows check if GUI-user has SELECT grant for a table.
|SELECT ON sys.DBA_Sys_Privs|Allows check if GUI-user has SELECT grant for a table.
|SELECT ON sys.DBA_Tables|Allows listing of table names for tables without SELECT grant (not included in All_Tables).
|SELECT ON sys.DBA_Tab_Columns|Allows listing of column names for tables without SELECT grant (not included in All_Tab_Columns).
|SELECT ON sys.DBA_Tab_Privs|Allows check if GUI-user has SELECT grant for a table.
|SELECT ON sys.gv_$Lock|Allows check for housekeeping if there are pending transactions. Accessed via synonym public.gv$Lock.
|SELECT ON sys.v_$Database|Get DB Info.
|SELECT ON sys.v_$Instance|Get DB version.
|SELECT ON sys.v_$Session|Allows DB session info in health check.

|===
If suitable an alternative for the detailed single grants may also be to grant 'SELECT ANY DICTIONARY' to MOVEX CDC's DB-user.

Instead of manually creating the DB user you can let MOVEX CDC itself create the schema owner for Oracle with all required grants by issuing:
[source]
docker run --rm \
  -e KAFKA_SEED_BROKER=/dev/null
  -e DB_TYPE=ORACLE
  -e DB_USER=hugo
  -e DB_PASSWORD=hugo
  -e DB_SYS_PASSWORD=oracle
  -e DB_URL=10.213.131.150:1521/ORCLPDB1
  ottogroupsolutionproviderosp/movex-cdc bundle exec rake ci_preparation:create_user

.Optional grants required to initially transfer table content in Oracle DB
[cols="~,~"]
|===
|Grant|Description

|SELECT ON <table>|Allows selection of table data for initial transfer to Kafka
|FLASHBACK ON <table>|Allows selection of table data by flashback query limited to the existing records at the current SCN of trigger creation +
Since the FLASHBACK grant alone does not allow the selection of data from a table without the SELECT grant, this requirement can also be satisfied by granting FLASHBACK ANY TABLE to MOVEX CDC's DB user.
|===

==== Kafka configuration ====
.Options for Kafka consumer
[cols="~,~,~"]
|===
|Option|Value|Description

|isolation-level|read_comitted|If not set to read_comitted the consumer will early read/consume messages of pending transactions that are possibly rolled back later by MOVEX CDC. Later successful processing of messages by MOVEX CDC may lead to duplicate occurrence of messages in consumer's stream.
|===

=== Configuring MOVEX Change Data Capture ===
You can configure the application either by defining config settings as environment variables or by storing configuration settings in a YML file and providing the location of this config file via RUN_CONFIG.

Environment variables overrides values from configuration file.

.Mandatory environment parameters for evaluation at appliction start
[cols="~,~"]
|===
|Variable|Description

|DB_PASSWORD|Password ofÂ DB_USER, aims also as password of user 'admin' for GUI-logon. Therefore also required for database without access control like SQLite.
|DB_TYPE|Defines the typ of observed database. Valid values: SQLITE, ORACLE
|DB_URL|Database-URL for JDBC Connect:
Example for Oracle: "MY_TNS_ALIAS" or "machine:port/service"
|DB_USER|Username of MOVEX CDC's DB schema in the observed database
|KAFKA_SEED_BROKER|Comma-separated list of seed-brokers for Kafka logon (Host:Port), Example: "kafka1.osp-dd.de:9092, kafka2.osp-dd.de:9092"
"/dev/null" for mocking of Kafka connection in tests (discard events instead of transfer to Kafka).
|===

.Optional environment parameters for evaluation at appliction start
[cols="~,~,~"]
|===
|Variable|Description|Default value

|DB_QUERY_TIMEOUT|Maximum runtime in seconds of database query. Monitors selection on table Event_Logs. All other SQL executions are monitored by socket timeout with twice this value. |600
|DB_SYS_PASSWORD|Password of DB admin user. Required only for additional maintenance tasks like creation of DB user by MOVEX CDC (ci_preparation:create_user)|
|ERROR_MAX_RETRIES|Maximum number of retries after error during transfer to Kafka|5
|ERROR_RETRY_START_DELAY|Number of seconds after error before first retry starts. This delay is tripled for each next retry.|20
|FINAL_ERRORS_KEEP_HOURS|Number of hours final errors are kept in table Event_Log_Final_Errors before erase them by housekeeping|240
|INFO_CONTACT_PERSON|Name and email of contact person for display at GUI home screen|
|INITIAL_WORKER_THREADS|Initial number of worker threads. Each worker threads has it's own connection to database and Kafka and operates independent on transferring events from local DB table to Kafka.|3
|JAVA_OPTS|Set Java options for jRuby runtime of the application. For example set to '-Xmx8192m' to allow MOVEX CDC to use up to 8GB of memory for Java heap memory.|'-Xmx<n>m' where n is 75% of the available memory
|KAFKA_COMPRESSION_CODEC|Compression codec used to compress transferred events. Valid values are: 'none' for not using compression or 'snappy' and 'gzip'. The other compression codecs for Kafka 'lz4' and 'tzstd' are not yet supported by MOVEX CDC.|gzip
|KAFKA_MAX_BULK_COUNT|Maximum number of messages to process within one bulk operation to Kafka. Higher values increases risk of unexpected errors like Kafka::MessageSizeTooLarge|1000
|KAFKA_SSL_CA_CERT|Path to CA certificate file in pem format|
|KAFKA_SSL_CLIENT_CERT|Path to client certifikate file in pem format|
|KAFKA_SSL_CLIENT_CERT_KEY|Path to client key in pem format|
|KAFKA_SSL_CLIENT_CERT_KEY_PASSWORD|Password for client key|
|KAFKA_TOTAL_BUFFER_SIZE_MB|Memory buffer size for Kafka message buffer in Megabyte. Maximum for the allocated memory for buffered Kafka messages before delivery. +
This amount of memory is per Thread so the maximum overall memory consumption for Kafka buffers is KAFKA_TOTAL_BUFFER_SIZE_MB * INITIAL_WORKER_THREADS. +
If the amount is not sufficient at runtime then the value of KAFKA_MAX_BULK_COUNT is automatically decreased by the application until it is according to the available memory.|100
|LOG_LEVEL|Log level of application (debug, info, warn, error)|info
|MAX_FAILED_LOGONS_BEFORE_ACCOUNT_LOCKED|Number of failed logons to GUI before the used user account will be locked and has to be unlocked by an admin user|3
|MAX_TRANSACTION_SIZE|Maximum number of messages for processing within one transaction (both DB and Kafka). May be overbooked up to twice the number for special circumstances.|10000
|MAX_SIMULTANEOUS_TABLE_INITIALIZATIONS|Maximum number of simultaneously processed initial transfers of table data after first trigger generation (number of tables)|5
|MAX_SIMULTANEOUS_TRANSACTIONS|Maximum number of transactions simultaneously processing inserts into table EVENT_LOGS without serialization. +
This value controls the setting for INI_TRANS for ORACLE.
Changing this setting requires that there are no pending transactions on table Event_Logs at next startup of the application container.
Otherwise error ORA-00054 is raised and application does not start. +
You should ensure that this value is higher than the expected maximum number of simultaneous transactions on table EVENT_LOGS (User transactions firing triggers + worker threads). +
Reaching this limit with the number of simultaneous pending transactions at one DB block may lead to mismatches in processing order of events for Oracle DB
because SELECT FOR UPDATE SKIP LOCK skips also unlocked records in DB blocks with full ITL (interested transaction list).
|60
|PARTITION_INTERVAL|Interval in seconds between partition changes for table EVENT_LOGS. +
Partition change is used to free already used storage after some seconds and keep the footprint of table EVENT_LOGS as small as possible. +
Relevant only if EVENT_LOGS is used partitioned. +
Changing this setting requires that there are no pending transactions on table Event_Logs at next startup of the application container.
Otherwise error ORA-00054 is raised and application does not start.
|60 seconds
|RAILS_MAX_THREADS|Maximum number of threads for the underlying Puma application server, should be set to greater than INITIAL_WORKER_THREADS + 30 if default is not sufficient|300
|RUN_CONFIG|Path and name of configuration file in YML format as alternative to configuration by environment variables|APP_ROOT/config/run_config.yml
|SECRET_KEY_BASE|Server side key used for encryption and signing of the JWT that is used for authentication|
|SECRET_KEY_BASE_FILE|Location of file with server side key used for encryption and signing of the JWT that is used for authentication|
|TNS_ADMIN|Directory of config file tnsnames.ora for resolution of Oracle DB aliases (File tnsnames.ora is usually mounted into Docker-Container). Valid for Oracle only.|
|TZ|Sets local timezone within the Docker-container of the applikation. Must be directly set as environment of container during 'docker run' like '-e TZ="Europe/London"', does not work from config file.|Europe/Berlin
|===


==== Setup SSL-connection to Kafka ====
Kafka supports encryption and authentication via SSL.
The required setup of Kafka for SSL is described at http://kafka.apache.org/documentation.html#security_ssl.
However, the certificates generated for Kafka are in JKS format, which the underlying library 'ruby-kafka' of MOVEX CDC does not support.
Luckily, it is possible to convert the generated files into X509 format.
A guide how to do conversion is here: https://github.com/zendesk/ruby-kafka/wiki/Creating-X509-certificates-from-JKS-format.

===== Steps to convert the keystore content generated by Kafka into the formats needed for MOVEX CDC =====
Preconditions for the next steps are the openssl command line tools "keytool", "openssl"
and optionally the GUI-tool "Keystore Explorer" (https://keystore-explorer.org)

Location and passwords are used as environment variables.

*1. Extract the alias name used in client keystore file*

Identify the second alias name other than 'caroot' and use this alias in next steps for $ALIAS.

[source]
keytool -list -v -keystore $KAFKA_CERT_DIR/kafka.client.keystore.jks -storepass $SSL_KEYSTORE_PASSWORD | grep -i alias

*2. Extract the signed client certificate*

[source]
keytool -noprompt -keystore $KAFKA_CERT_DIR/kafka.client.keystore.jks -exportcert -alias $ALIAS -rfc -storepass $SSL_KEYSTORE_PASSWORD -file client_cert.pem

*3. a. Extract the client key with command line tools*

New client certificate key password becomes the same like source keystore password in this example.

[source]
keytool -noprompt -srckeystore $KAFKA_CERT_DIR/kafka.client.keystore.jks -importkeystore -srcalias $ALIAS -destkeystore cert_and_key.p12 -deststoretype PKCS12 -srcstorepass $SSL_KEYSTORE_PASSWORD -storepass $SSL_KEYSTORE_PASSWORD

[source]
openssl pkcs12 -in cert_and_key.p12 -nocerts -nodes -passin pass:$SSL_KEYSTORE_PASSWORD -out client_cert_key.pem

After generation open the file 'client_cert_key.pem' in an editor and remove all attributes at top so the file content now starts with "-----BEGIN PRIVATE KEY-----".

*3. b. Extract the client key with Keystore explorer as alternative to 3. a.*

* Open file $KAFKA_CERT_DIR/kafka.client.keystore.jks im Keystore Explorer
* Choose the alias identified in step 1
* Choose menu 'Export' / 'Export private key', use format 'openssl'

*4. Extract CA certificate*

[source]
keytool -noprompt -keystore $KAFKA_CERT_DIR/kafka.client.keystore.jks -exportcert -alias CARoot -rfc -file ca_cert.pem -storepass $SSL_KEYSTORE_PASSWORD

Now you are prepared with the four values needed to configure SSL connection in MOVEX CDC.

=== Running MOVEX Change Data Capture ===
The application is provided as Docker-Image by:
[source]
docker pull ottogroupsolutionproviderosp/movex-cdc

==== Start Docker container ====
You can run the this image like:
[source]
docker run -p 8080:8080 \
  --stop-timeout=120 \
  -e RUN_CONFIG=/etc/run_config.yml \
  -v /my_local_dir/run_config.yml:/etc/run_config.yml \
  ottogroupsolutionproviderosp/movex-cdc

The web-GUI would be available by http://localhost:8080 in this case.
It is recommended to place an own reverse proxy nearby for SSL encryption.

==== Stop Docker container ====
To stop the Docker container you should provide a timeout (at "docker run" or with "docker stop") that allows MOVEX CDC to gracefully shutdown all worker threads before Docker terminates hard with "kill -9".

 docker stop -t 120 <container name/id>

==== Uniqueness of container instances ====
Depending on the database type you may run multiple MOVEX CDC container instances at one database or not.

.Multiple instances allowed for MOVEX CDC
[cols="~,~,~"]
|===
|DB type|Multiple instances with same configuration (same DB schema for MOVEX CDC)|Multiple instances with different configuration (different MOVEX CDC schemas, different Kafka targets)

|SQLite
|Not allowed: No synchronization between multiple instances exist
|Not allowed: No config-specific trigger names are used
|ORACLE
|Possible: Messages to transfer to Kafka are selected with FOR UPDATE.
|Possible: Trigger names contain numeric hash value of MOVEX CDC's owner schema. +
Therefore multiple triggers from several independent MOVEX CDC configurations at one table are possible.
|===

WARNING: But be aware if running multiple container instances of MOVEX CDC on the same database schema (same configuration) simultaneously: +
MOVEX CDC cannot guarantee the exact order of messages with key for transfer to Kafka in this case!


=== Configuration of capturing with GUI ===
TODO: Describe GUI workflow

==== Managing users, schemas and rights ====
Menu "Users" shows the already created named users. Initially there is always a predefined user 'admin'. +
Users are identified by E-Mail.
For authentification at logon one DB-User is associated to each application user of MOVEX CDC, the password of this DB-user is used for logon.

The application user is authorized for certain schemas for which tables can be tagged for event capturing.
This schemas can be picked from the list of schemas where the user has select grants at at least one table of this schema.

==== Configuring events to capture at column level ====
This dialog shows:

* schemas for which the application user has the right to configure (set in user configuration)
* already configured tables of a schema (limited to tables where the user has SELECT grants for)
* columns of a configured table with marks for Insert/Update/Delete-trigger

Possible configuration actions are:

* add tables to configuration for a schema (only possible for tables where the user is allowed to select from)
** modify topic name per table
** choose a value for Kafka key (None / Primary key / Fixed value / Transaction-ID )
** decide if transaction-ID should be recorded in events (adds approx. 0.3 ms per triggering SQL execution)
** decide wether the current content of the table should be initially transferred to Kafka at trigger deployment or nor
* modify triggering of change events per column
** Define the operations (insert/update(delete) to capture for a column
** Define optional filter conditions per operation +
This filter conditions my rely on column values inside the trigger and may also contain subselects to other tables +
Example `:new.Amount > 12 AND 2=(SELECT Company FROM Other_Table WHERE ID=:new.Other_Table_ID)`

NOTE: The configuration in this screen is not user-specific. Each table/column configuration exists only once and can be manipulated by several permitted users.

==== Generation of database triggers ====

=== Troubleshooting ===
==== Common error traps ====
===== Kafka =====
List of Kafka error codes is avaliable here: https://kafka.apache.org/protocol#protocol_error_codes

.possible problems accessing or using Kafka
[cols="~,~,~"]
|===
|Error|Description|Solution

|Kafka::UnknownError: Unknown error with code 53
|TRANSACTIONAL_ID_AUTHORIZATION_FAILED +
The transactional id used by MOVEX CDC is not authorized to produce messages
|Explicite authorization of transactional id is required, optional as wildcard: +
kafka-acls --bootstrap-server localhost:9092 --command-config adminclient-configs.conf
--add --transactional-id * --allow-principal User:* --operation write
|Kafka::UnknownError: Unknown error with code 87
|INVALID_RECORD +
This record has failed the validation on broker and hence will be rejected.
|Possible reason: Log compaction is activated for topic (log.cleanup.policy=compact) but events are created by MOVEX CDC without key. +
Prevent from sending 'tombstone events' without key in this case.
|===

===== Oracle-DB =====
* If TNS alias is used for DB_URL but no tnsnames.ora available at TNS_ADMIN then the JDBC driver treats the TNS alias as host:port:sid with several possible error messages (host does not exist etc.)
* Oracle's number format for values between -1 and 1 is not JSON-compatible (0,123 = .123).
Up to Rel. 12.2 the patch https://support.oracle.com/epmos/faces/PatchResultsNDetails?_adf.ctrl-state=19z17iq454_4&releaseId=600000000018520&requestId=21922926&patchId=27486853&languageId=0&platformId=226&searchdata=%3Ccontext+type%3D%22BASIC%22+search%3D%22%26lt%3BSearch%26gt%3B%26lt%3BFilter+name%3D%26quot%3Bpatch_number%26quot%3B+op%3D%26quot%3Bis%26quot%3B+value%3D%26quot%3B27486853%26quot%3B%2F%26gt%3B%26lt%3B%2FSearch%26gt%3B%22%2F%3E&_afrLoop=164497543848765[27486853] is needed to generate valid JSON in this case.

===== Docker / Kubernetes =====
The Docker container of MOVEX CDC produces a continous log output which can become quite large over time.
You should ensure that logfile size of the Docker container is not unlimited because this may end up in full filesystem. +
For Docker you can configure this behaviour in /etc/docker/daemon.json like this:

[source]
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  }
}



=== Statistics ===
Throughput values of the application are cumulated in the database table "Statistics".
For table, operation and time period several values are recorded.

.throughput parameters recorded in Statistics
[cols="~,~"]
|===
|Column name |Description

|Events_Success|Number of successful processed events
|Events_Delayed_Errors|Number of erroneous single event processings ending in another retry after delay
|Events_Final_Errors|Number of erroneous single event processings ending in final error after retries
|Events_D_and_C_Retries|Number of additional event processings due to divide&conquer retries
|Events_Delayed_Retries|Number of additional event processings due to delayed retries
|===

At first this values are cumulated for each minute. Later on statistics data will be compressed for greater time periods:

* After 14 days values per minute are compressed to values per hour
* After 3 months values per hour are compressed to values per day

Compression is executed once a day as background job in the application.

== Technical implementation ==
=== Structure of Kafka messages ===
MOVEX CDC creates Kafka messages with JSON-formatted content. +
Depending on table configuration Kafka messages may contain an additional key value which drives the assignment of messages to partitions (messages with same key are stored in the same partition).

.Value conversion from database column to JSON value
[cols="~,~,~"]
|===
|JSON representation|Example|Oracle data types

|Number|45.23|BINARY_DOUBLE, BINARY_FLOAT, FLOAT, NUMBER
|String|"Value"|CHAR, CLOB, NCHAR, NCLOB, NVARCHAR2, LONG, ROWID, UROWID, VARCHAR2
|String|"2020-02-21T12:07:43"|DATE
|String|"2020-02-21T12:07:43,396153000"|TIMESTAMP
|String|"2020-02-21T12:07:43,396142000+00:00"|TIMESTAMP WITH TIME ZONE
|String|"90FF"|RAW
|===


.Field names used in Kafka message
[cols="~,~"]
|===
|Fieldname|Explanation

|id|consecutive unique message ID, describes the order of message creation at database trigger level
|schema|schema name of database table
|tablename|name of database table
|operation|kind of triggering database operation (INIT / INSERT / UPDATE / DELETE)
|dbuser|database user who run the triggering operation
|timestamp|detailled timestamp of triggering event
|transaction_id|unique ID of database transaction (optional)
|old|values of observed columns before triggering change event
|new|values of observed columns after triggering event
|===

==== Example of initial load message ====

[source, json]
{
  "id": 23423274179,
  "schema": "EINKAUF",
  "tablename": "HUGO",
  "operation": "INIT",
  "dbuser": "MEYER",
  "timestamp": "2020-02-21T12:07:43,396142+00:00",
  "transaction_id": null,
  "new": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": null
  }
}

==== Example of INSERT message ====

[source, json]
{
  "id": 23423274179,
  "schema": "EINKAUF",
  "tablename": "HUGO",
  "operation": "INSERT",
  "dbuser": "MEYER",
  "timestamp": "2020-02-21T12:07:43,396142+00:00",
  "transaction_id": "9.5.374674",
  "new": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": null
  }
}

==== Example of UPDATE message ====

[source, json]
{
  "id": 234232741379,
  "schema": "EINKAUF",
  "tablename": "HUGO",
  "operation": "UPDATE",
  "dbuser": "MEYER",
  "timestamp": "2020-02-21T12:07:43,396142+00:00",
  "transaction_id": "9.5.374674",
  "old": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": null
  },
  "new": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAACAAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": null
  }
}

==== Example of DELETE message ====
[source, json]
{
  "id": 2342327412279,
  "schema": "EINKAUF",
  "tablename": "HUGO",
  "operation": "DELETE",
  "dbuser": "MEYER",
  "timestamp": "2020-02-21T12:07:43,396142+00:00",
  "transaction_id": null,
  "old": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": null
  }
}


=== Module overview ===
image:module_overview.svg[format=svg,opts=inline]

==== Activities at application startup ====

The following things are executed at startup of application / docker container if necessary:

* The needed data structures in MOVEX CDC's DB schema (defined by DB_USER) are created or updated
* The initial application user "admin" is created for GUI logon with link to the DB_USER for authentication
** For initial GUI logon with user "admin" the password is the DB-passwort of MOVEC CDC's DB-user (DB_PASSWORD)
** The GUI user "admin" acts as supervisor with the authorization to administrate further user accounts

==== Generation of database triggers

===== Creating triggers based on configuration
===== Initial transfer of table content at first trigger generation
If requested in table config, after generation of trigger a job will be created for transfer of the already existing records of a table to Kafka. +
For each record in the table existing at the time of trigger creation an insert-like event will be transferred to Kafka.
The field 'operation' is marked INIT instead of INSERT to be able to distinguish between real insert events and initial load events.
The table's filter condition for insert operation as well as the filter condition for initialization are considered. +
This jobs are queued and processed deferred asynchronously.
The maximum number of simultaneously processed table initialization jobs is limited by the environment setting MAX_SIMULTANEOUS_TABLE_INITIALIZATIONS.

Precondition for initial transfer of table data is that MOVEX CDC's DB user is allowed to read this table by SELECT, because initial transfer is done directly by selecting from table, not by trigger execution.

.Techniques used to determine rows for initial transfer
[cols="~,~"]
|===
|Database|Technique

|Oracle|Flashback table by SCN is used to select from the table in it's state directly after insert trigger check/creation.

To be 100% sure that each record is transferred either by initial transfer or by trigger event the following conditions must be valid: +
- There should not be pending transactions for this table at the time of trigger creation because this uncommited records are not catched later by "SELECT ... AS OF SCN" +
- There should not be insert operations during the trigger creation because this may result in duplicate insert events from initialization and trigger

|===


==== Transfer of triggered events to Kafka ====
An consecutive ID is used to define the order of message creation at trigger level. +
This ID allows the reconstruction of the original order of messages in Kafka even if using topics with multiple partitions.

NOTE: For Oracle-DB: If using RAC this ID represents the original order only per RAC-instance because a cached sequence is used for value generation.

Message creation in Kafka is done by the application with multiple concurrent threads. +
Each transfer thread has it's own connection to source database as well as to Kafka. +
To guarantee the original creation order of events also during transfer to Kafka,
exactly one of MOVEX CDC's transfer threads is responsible for transfer of all events with the same key. +
That means, events without a key can be transferred by every thread, events with a key are transferred by one particular thread determined by a hash value of the key and a modulo operation.

===== Error handling during transfer =====
Transferring is done with bulk operations against database and Kafka.
If the transfer operation fails the bulk size would be reduced (divide & conquer) until a single event is processed in it's own transaction. +
If this single processing still fails then the event is marked in Table Event_Logs and suspended for processing for the time defined by ERROR_RETRY_START_DELAY.
After a number of not successful retries (defined by ERROR_MAX_RETRIES) the erroneous event is moved to table 'Event_Log_Final_Errors'.

Events moved to final error table can be rescheduled by API function: _TODO: mark API function_

If no further action happens then this event is erased from table 'Event_Log_Final_Errors' by a houskeeping process after FINAL_ERRORS_KEEP_HOURS.

Reasons for transfer errors can be for example:

* non-existing Kafka topic
* exceeding the maximum event size for Kafka topic
* event without key but log compaction set for Kafka topic


==== Houskeeping ====

==== Health check ====
The healthcheck service is available at:

 http://<MOVEX CDC URL>/health_check

It can be called maximum once a second.
The http-response contains a JSON-object with detailled informations.
There's no authentification needed for execution of health check.
The response status code contains the health status of the running instance:

- 200 (ok): Health Check o.k., the configured number of worker threads exists and is functional.
- 409 (conflict): Health check recognized a problem in operation
- 500 (internal server error): Technical problem during processing of health check request or called too frequently (further details in response body)

==== Export and import of configuration data ====
The content of configuration tables can be exported as a consistent JSON document.
This JSON document can also be imported to a MOVEX-CDC instance. +
This way the configuration data can be stored as a backup outside the database and MOVEX-CDC instance.
Import and export requires authentication as user with admin rights.
This function is available in MOVEX-CDC's GUI at menu 'Administration/Config exchange' and also as raw http API.

===== Export of configuration data =====
The export to a JSON document can be executed for a particular DB schema or for all schemas.
It exports schemas, tables, columns, conditions, schema rights and users.

This example exports the whole configuration data into a file +
[source]
curl -X GET -H "Authorization: \
`curl -d "email=admin&password=<my_password>" http://localhost:8080/login/do_logon | \
jq .token | sed -e 's/^"//' -e 's/"$//'`" \
http://localhost:8080/import_export/export > movex-cdc-config.json

With schema name in parameter 'schema' the exported is limited to a particular schema. +
Data of all configured users is included in the JSON document in both cases.

===== Import of schema configuration data =====
The import from JSON data can be done for the whole content or you may pick only one schema to import. +

This example imports a particular schema from a JSON document with several schemas:
[source]
curl -X POST -H "Authorization: \
`curl -d "email=admin&password=<my_password>" http://localhost:8080/login/do_logon | \
jq .token | sed -e 's/^"//' -e 's/"$//'`" \
-d "json_data=`cat movex-cdc-config.json | sed 's/"/\\"/g'`" \
http://localhost:8080/import_export/import

===== Import of user configuration data =====
If importing one or all schemas from JSON file only users with rights for this schemas are created if they don't yet exist in DB.
Missing users are created with locked account in this case to avoid unwanted security issues.

To import all users with all their attributes from JSON document there's a separate function.


=== API endpoints ===
Most of the API endpoints are useful only when called from GUI, but several of this API endpoints may also be useful for calling from outside the application. +
API Responses are JSON objects.

.API endpoints for additional usage from outside the application
[cols="~,~,~,~,~"]
|===
|Verb|URL|Parameter|Response|Description

|POST|/login/do_logon|email, password|token|Validate user authentication, get JWT token for authentication/authorization of following requests
|GET|/health_check|no|JSON object with several application status info|ask health status (200=ok) and get some condensed status information
|GET|/health_check/log_file|no|current log file of application|Download log file of MOVEX CDC application. +
Requires valid user JWT in request header.
|POST|/server_control/set_log_level|log_level (DEBUG, INFO, WARN, ERROR, FATAL)|no|Set log level of server instance, requires valid admin JWT in request header
|POST|/server_control/set_max_transaction_size|max_transaction_size (1..infinity)|no|Set the number of events to process within on DB and Kafka transaction, requires valid admin JWT in request header
|POST|/server_control/set_worker_threads_count|worker_threads_count (0..200)|no|Set number of active worker threads, requires valid admin JWT in request header
|POST|/server_control/terminate|no|no|Graceful shut down the current container instance of MOVEX CDC by sending SIGTERM to the application, requires valid admin JWT in request header
|GET|/import_export/export|schema (Limit export to a single schema, optional)|JSON object|Export configuration data of all or a particular schema (users, schemas, tables, columns, conditions, schema rights, users) as JSON object
|POST|/import_export/import|JSON object, schema (Limit exports to a single schema, optional)|no|Import configuration data for users and schemas. Each user / schema contained in JSON object creates/replaces the configuration data in the applications config tables
|POST|/import_export/import_all_users|JSON object|no|Import the complete configuration data for users from JSON object. The schema import in contrast imports only the users that are necessary for dependencies with locked account.
|===

==== Using API endpoints from command line ====
You can use curl or wget to call API funktions with valid autorization by email and password. +
Example is for setting log level to DEBUG, adjust hostname, port, email and password to your needs. +
Needed tools are curl, jq, sed. +
Steps are:

* authenticate with valid user and get JWT token for next steps
* call API methode with use of JWT

To use this example replace the values for email, password, host and port with yours:
[source]
curl -X POST -H "Authorization: \
`curl -d "email=admin&password=<my_password>" http://localhost:8080/login/do_logon | \
jq .token | sed -e 's/^"//' -e 's/"$//'`" \
-d "log_level=ERROR" \
http://localhost:8080/server_control/set_log_level

=== Entity relationship model ===
image:er_model.svg[format=svg,opts=inline]

=== Security considerations ===
==== Encryption of network traffic ====
===== Encrpytion of DB connection =====
Connections to Oracle-DB are unencrypted until now.
Encryption of DB connection will be default soon after release of https://github.com/rsim/oracle-enhanced/pull/2284[this pull request] for the underlying DB adapter. +
Precondition for DB network encryption is that the DB server ist configured in sqlnet.ora with at least "SQLNET.ENCRYPTION_SERVER = ACCEPTED".

===== Encrpytion of Kafka connection =====
CAUTION: TODO: Describe preconditions for encrypted traffic between MOVEX CDC and Kafka

===== SSL encryption for https access on GUI =====
There is no SSL/HTTPS encryption for the GUI of MOVEX CDC out of the box. +
To ensure encypted HTTP traffic you should place MOVEX CDC behind a reverse proxy or ingres controller with SSL encryption. +

CAUTION: TODO: Example config for MOVEX CDC behind nginx with docker compose should be added

==== Access authentication for GUI ====
* Users authenticate at logon with the password of the corresponding DB user
* A JWT token is created at GUI logon and used for subsequent API calls
* This JWT token is signed by a key that is stored in the local file config/secrets.yml.
This key can be defined by several ways:
** The key is generated at first startup if neither SECRET_KEY_BASE nor SECRET_KEY_BASE_FILE is given
** The key is given by environment variable SECRET_KEY_BASE
** The key ist given in a file pointed to by environment variable SECRET_KEY_BASE_FILE
* Usually the generated key should be sufficient. This key changes only at recreation of Docker container.

==== Lock user account after multiple failed logons ====
User account is locked after 3 subsequent failed logon tries. +
Unlocking a locked account is possible via GUI for admin users.

==== Suppress frequent access ====
* Email/password check at /login/do_logon is delayed for up to 5 seconds if subsequent logon requests occur within 5 seconds
* Subsequent calls to /health_check are rejected within the same second

==== Isolation between the production schemas of the DB and MOVEX CDC's schema ====
MOVEX CDC requires an own schema at the database. This schema must not contain any foreign structures.
All database changes made by MOVEX CDC are isolated to this schema (including te generated triggers).
The owner of MOVEX CDC's DB schema requires only a minimum set of rights on foreign objects, especially no right to read the full table content (except if initialization is requested).

==== Restricted definition of triggers ====
There might be a security gap if users may define trigger on tables where they don't have read rights.
This way they could possibly read hidden table content via Kafka. +
Therefore only tables are accessible for trigger definition in the GUI where the DB user associated with the application user has at least read rights.

== Frequently asked questions ==
.FAQ will be completed over time with all upcoming questions that are not answered befor
[cols="~,~"]
|===
|Question|Answer

|Do I have to redeploy triggers if I want to change the topic of a table|
The target topic for a table is read from current configuration before transferring events to Kafka. +
Therefore there is no need to redeploy the triggers in this case. +
Each worker thread caches the topics of a table or schema for max. 60 seconds before refreshing it with the current configuation.
If want to exactly define the timestamp of change, then: +
- Set the worker count to 0 +
- Wait until all worker threads have finished. You can check this by health check. +
- change the topic in GUI +
- set the worker count to > 0. Now the new topic is used for transfer to Kafka.

|===
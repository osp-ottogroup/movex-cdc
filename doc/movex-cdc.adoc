= image:osp.png[float="left" width=200 ] MOVEX Change Data Capture  =
Author: Peter Ramm ( Peter.Ramm@ottogroup.com )
:Author Initials: PR
:toc:
:toclevels: 4
:icons:
:imagesdir: ./images
:numbered:
:sectnumlevels: 6
:homepage: https://www.osp.de
:title-logo-image: osp.png
:description: Solution for change data capture from Oracle to Kafka
:keywords: Oracle, Kafka, Change Data Capture, CDC, Trigger

TIP: This page is not yet fully completed and still under construction.
Please open an https://gitlab.com/otto-group-solution-provider/movex-cdc/-/issues[issue] if you miss some information.


== Introduction ==

This documentation describes the product MOVEX Change Data Capture which is hosted at https://gitlab.com/otto-group-solution-provider/movex-cdc.

=== Purpose ===
**********************************************************************
Captures data change events (Insert/Update/Delete) in Oracle and other databases and immediately transfers the data changes to Kafka event hub.
**********************************************************************


MOVEX CDC allows a system-wide identical and redundancy-free capture of change events on database tables. +
The Kafka-Cluster , which is supplied with events by MOVEX CDC, is responsible for the provision and distribution of events to any consumers (Publish & Subscribe).

=== Concept ===
**********************************************************************
* Database triggers are used to capture change events.
* The configuration for the tables and columns to observe by triggers is stored local in the database in MOVEX CDC's schema.
* This configuration can be maintained manually by the application's web-GUI but can also be loaded as JSON file
(configuration as code in revision control). +
* The database triggers are generated based on this configuration data via web-GUI or API call.
**********************************************************************

Synchronous processing and storage of the trigger events is initially performed locally in the database, without further dependencies on external systems.
The further transmission of the events to Kafka is asynchronous to the trigger processing.

image::event_flow.svg[format=svg,opts=inline]

The focus is on resource-conserving yet stable and high-performance processing,
low complexity in the operation of the solution and minimal intervention in the operation of the database.
In particular, compared with alternative solutions such as Oracle Golden Gate, Quest Shareplex or Red Hat Debezium,
it is not necessary to drastically increase the retention period of the DB online transaction log.

==== Using Kafka keys to ensure sequential order of messages ====
For Kafka consumers the original sequence of messages is guaranteed only for messages consumed from the same partition of a topic. +
Therefore you must place messages within the same partition of a topic if you want to consume them in original order. +
Kafka has the concept of message keys for that. Kafka ensures that messages with the same key value are placed in the same partition and this way are consumed in original sequence.

MOVEX CDC supports four kinds of message keys for Kafka that can be defined by GUI at table level:

* *No message key*: Messages are placed randomly in partitions. MOVEX CDC transmits events in multiple simultaneous threads, therefore sequential order is not guaranteed,
even if the target topic has only one partition.
* *Fixed value*: All change events of a table are placed in the same single partition.
* *Primary key values*: Ensures that the change history of a single DB record is always consumed in original sequence
* *Transaction-ID*: Ensures that all events of a particular DB transaction can be consumed in original sequence

Events with the same key value are always transferred by exactly one transfer worker thread (to ensure the sequence).
That means the strategy of key creation influences the horizontal scalability over multiple worker threads and this way the overall transfer bandwith of your MOVEX CDC instance.

NOTE: For Oracle-DB: If using RAC the sequential event ID represents the original order only per RAC-instance because a cached sequence is used for value generation.

==== Uniqueness of incoming events in Kafka ====
* MOVEX CDC works with transactions at both DB and Kafka.
* Each change event recorded in DB is transferred to Kafka and committed there exactly once.
* A non commited transmission to Kafka can occur several times if transfer is repeated on error. +
Caution: Kafka distinguishes between read_uncommited and read_commited when consuming.
* Each event has a unique sequential event ID created by a DB sequence while storing event in trigger.
* Transactional coupling between the two resources DB and Kafka is implemented with two nested transactions inside the MOVEX CDC application.
There is no XA or 2-phase commit coupling of the two transactions.





=== Differentiation from other solutions for CDC ===
There are a number of existing solutions for change capture, commercial as well as open source.
Most of them are based on processing of DB's transaction log. +
Using transaction log for CDC ensures that no additional effort is loaded on the primary transactions,
so processing the change events is completely asynchroneous. +
But this solutions also mean:

* Covering outages of CDC target (Kafka) requires later processing of transaction log when CDC target systems become available again
* Therefore you have to preserve the transaction log in space for the longest expected outage of the CDC target, if you expect to continue processing automatically after CDC target system outage
* Including weekend, public holidays and some time for troubleshooting this regularly requires to preserve the DB transaction log in place for at least three days
* Especially for Oracle you have to activate SUPPLEMENTAL LOGGING which significantly increases transaction log sizes
* If you only need a small amount of change events from large transaction processing systems then the effort in dealing with transaction logs becomes complex and expensive compared to what you actually want.

This is the case where MOVEX CDC comes into play. +
Accepting the synchroneous overhead of triggers in business transactions the solution is sized for the expected amount of observed change events independent from the total transaction throughput of the entire database.

.Other common existing solutions for change data capturing and transfer to Kafka
[cols="~,~"]
|===
|Product|Info

|https://debezium.io[Debezium]|Open source solution for several database systems. +
Works with https://docs.oracle.com/database/121/XSTRM/xstrm_intro.htm#XSTRM1086[XStream API] (requires Golden Gate license for consumer) or directly by LogMiner for Oracle.
|https://docs.oracle.com/goldengate/c1230/gg-winux/index.html[Oracle Golden Gate]|
Commercial solution, requires licensing of producer and consumer
|https://www.quest.com/documents/shareplex-for-kafka-target-datasheet-144821.pdf[Quest SharePlex]|
Commercial solution, processes redo log files.
|https://docs.confluent.io/kafka-connect-oracle-cdc/current/index.html[Oracle CDC Source Connector for Confluent Platform:]|
Commercial solution, based on Logminer function.
Not yet functioning for Oracle 19c.
Requires supplemental logging in Oracle DB.
|===

=== Supported databases ===

==== Oracle Database ====
Oracle Databases are supported for release 12.1. and higher.

===== Enterprise or Express Edition with Partitioning Option =====
MOVEX CDC works best if Partitioning Option is available for your database in Enterprise or Express Edition.
Interval partitioning of table Event_Logs is used in this case which ensures automatic shrinking to minimum needed storage footprint. +

===== Standard Edition or Enterprise Edition without Partitioning Option =====
MOVEX CDC also works without partitioning,
but in this case there are some disadvantages:

- Peak usage increases high water mark in table Event_Logs, the claimed space is not freed after processing
- Because read access with full table scan is not suitable in this case due to the unpredictable size of the table, an index on column ID is placed for the non-partitioned table Event_Logs
- This index ensures processing troughput, but a tiny risk is remaining for wait szenarios at index block split operation under heavy concurrent transactions that are executing MOVEX CDC's triggers.


==== SQLite ====
SQLite is used as development database for MOVEX CDC. There might be no useful production use case but it works.

==== PostgreSQL, Microsoft SQL Server ====
Support for PostgreSQL and MS SQL Server is planned in the future. +
The implementation depends on achievable benefits in application and operation compared to simply using the existing open source log-based solution https://debezium.io[Debezium].

== Operation ==
=== Preconditions for usage ===
==== Sizing of server instance for MOVEX Change Data Cature ====
The application runs on one CPU and 4 GB of memory with it's default settings.
But for higher number of worker threads and/or larger memory buffer size you should increase the number or CPUs and memory according. +
By default MOVEX CDC uses up to 75% of the available memory.
If you want to limit the maximum memory used by MOVEC CDC then set JAVA_OPTS=-Xmx to the desired value (like JAVA_OPTS=-Xmx4096m for 4 GB ).


==== Database schema for MOVEX Change Data Cature  ====
The application needs it's own database schema at the observed database. +
This schema contains configuration tables as well as the buffered (not yet transferred) events. +
Storage quotas for this schema should allow storage of buffered events as long as the longest possibly expected outage of Kafka that should be covered without restrictions to the business transactions.

Schema objects needed for operation (tables, indexes, views) are created by MOVEX CDC itself at the first startup.
Also possible DB structure changes for future releases of MOVEX CDC are detected itself at first startup of the new release and are fixed by executing the needed transformation SQLs.

==== Rights for the owner of MOVEX CDC's DB schema ====
The owner of the schema requires some preconditions/grants at the database as well as quota on its default tablespace.
The existence of this grants is checked at application start.

To ensure sufficient user rights the schema owner for MOVEX CDC can also be created by the application itself with given DB admin credentials.

===== Rights for the owner of MOVEX CDC's DB schema: Oracle =====

.Minimum grants required to operate MOVEX CDC with Oracle DB
[cols="~,~"]
|===
|Grant|Description

|CONNECT|Allows establishing session
|CREATE ANY TRIGGER|Allows creation and dropping of triggers in foreign schemas of database
|CREATE VIEW|Allows creation of views in MOVEX CDC's DB schema
|RESOURCE|Allows creation of tables in own schema
|SELECT ON sys.DBA_Constraints|For primary key info of table.
|SELECT ON sys.DBA_Cons_Columns|For primary key info of table.
|SELECT ON sys.DBA_Role_Privs|Allows check if GUI-user has SELECT grant for a table.
|SELECT ON sys.DBA_Sys_Privs|Allows check if GUI-user has SELECT grant for a table.
|SELECT ON sys.DBA_Tables|Allows listing of table names for tables without SELECT grant (not included in All_Tables).
|SELECT ON sys.DBA_Tab_Columns|Allows listing of column names for tables without SELECT grant (not included in All_Tab_Columns).
|SELECT ON sys.DBA_Tab_Privs|Allows check if GUI-user has SELECT grant for a table.
|SELECT ON sys.gv_$Lock|Allows check for housekeeping if there are pending transactions. Accessed via synonym public.gv$Lock.
|SELECT ON sys.v_$Database|Get DB Info.
|SELECT ON sys.v_$Instance|Get DB version.
|SELECT ON sys.v_$Session|Allows DB session info in health check.

|===
If suitable an alternative for the detailed single grants may also be to grant 'SELECT ANY DICTIONARY' to MOVEX CDC's DB-user.

Instead of manually creating the DB user you can let MOVEX CDC itself create the schema owner for Oracle with all required grants by issuing:
[source]
docker run --rm \
  -e KAFKA_SEED_BROKER=/dev/null
  -e DB_TYPE=ORACLE
  -e DB_USER=hugo
  -e DB_PASSWORD=hugo
  -e DB_SYS_PASSWORD=oracle
  -e DB_URL=10.213.131.150:1521/ORCLPDB1
  ottogroupsolutionproviderosp/movex-cdc bundle exec rake ci_preparation:create_user

.Optional grants required to initially transfer table content in Oracle DB
[cols="~,~"]
|===
|Grant|Description

|SELECT ON <table>|Allows selection of table data for initial transfer to Kafka
|FLASHBACK ON <table>|Allows selection of table data by flashback query limited to the existing records at the current SCN of trigger creation +
Since the FLASHBACK grant alone does not allow the selection of data from a table without the SELECT grant, this requirement can also be satisfied by granting FLASHBACK ANY TABLE to MOVEX CDC's DB user.
|===

==== Kafka configuration ====
.Options for Kafka consumer
[cols="~,~,~"]
|===
|Option|Value|Description

|isolation-level|read_comitted|If not set to read_comitted the consumer will early read/consume messages of pending transactions that are possibly rolled back later by MOVEX CDC. Later successful processing of messages by MOVEX CDC may lead to duplicate occurrence of messages in consumer's stream.
|===

=== Configuring MOVEX Change Data Capture ===
You can configure the application either by defining config settings as environment variables or by storing configuration settings in a YML file and providing the location of this config file via environment variable RUN_CONFIG.

Environment variables overrides values from configuration file.

.Mandatory environment parameters for evaluation at appliction start
[cols="~,~"]
|===
|Variable|Description

|DB_PASSWORD|Password of DB_USER, aims also as password of user 'admin' for GUI-logon. Therefore also required for database without access control like SQLite.
|DB_TYPE|Defines the typ of observed database. Valid values: SQLITE, ORACLE
|DB_URL|Database-URL for JDBC Connect:
Example for Oracle: "MY_TNS_ALIAS" or "machine:port/service"
|DB_USER|Username of MOVEX CDC's DB schema in the observed database
|KAFKA_SEED_BROKER|Comma-separated list of seed-brokers for Kafka logon (Host:Port), Example: "kafka1.osp-dd.de:9092, kafka2.osp-dd.de:9092"
"/dev/null" for mocking of Kafka connection in tests (discard events instead of transfer to Kafka).
|===

.Optional environment parameters for evaluation at appliction start
[cols="~,~,~"]
|===
|Variable|Description|Default value

|DB_DEFAULT_TIMEZONE|Timezone value for internal DB timestamp, used for correct timezone setting of event timestamp. Default should be overwritten only if DB timezone settings are incorrect. E.g. "+00:00" for GMT.|Internal time zone setting of DB
|DB_QUERY_TIMEOUT|Maximum runtime in seconds of database query. Monitors selection on table Event_Logs. All other SQL executions are monitored by socket timeout with twice this value. |600
|DB_SYS_PASSWORD|Password of DB admin user. Required only for additional maintenance tasks like creation of DB user by MOVEX CDC (ci_preparation:create_user)|
|ERROR_MAX_RETRIES|Maximum number of retries after error during transfer to Kafka|5
|ERROR_RETRY_START_DELAY|Number of seconds after error before first retry starts. This delay is tripled for each next retry.|20
|FINAL_ERRORS_KEEP_HOURS|Number of hours final errors are kept in table Event_Log_Final_Errors before erase them by housekeeping|240
|INFO_CONTACT_PERSON|Name and email of contact person for display at GUI home screen|
|INITIAL_WORKER_THREADS|Initial number of worker threads. Each worker threads has it's own connection to database and Kafka and operates independent on transferring events from local DB table to Kafka.|3
|JAVA_OPTS|Set Java options for jRuby runtime of the application. For example set to '-Xmx8192m' to allow MOVEX CDC to use up to 8GB of memory for Java heap memory.|'-Xmx<n>m' where n is 75% of the available memory
|KAFKA_COMPRESSION_CODEC|Compression codec used to compress transferred events. Valid values are: 'none' for not using compression or 'snappy' and 'gzip'. The other compression codecs for Kafka 'lz4' and 'tzstd' are not yet supported by MOVEX CDC.|gzip
|KAFKA_MAX_BULK_COUNT|Maximum number of messages to process within one bulk operation to Kafka. Higher values increases risk of unexpected errors like Kafka::MessageSizeTooLarge|1000
|KAFKA_SASL_PLAIN_PASSWORD|Password for authentication with SASL_PLAIN or SASL_SSL|
|KAFKA_SASL_PLAIN_USERNAME|Username for authentication with SASL_PLAIN or SASL_SSL|
|KAFKA_SSL_CA_CERT|Path to CA certificate file in pem format. Use single file path or multiple file paths separated by comma. One file may contain multiple certificates.|
|KAFKA_SSL_CA_CERTS_FROM_SYSTEM|Use system CA certificates instead of providing your own's by KAFKA_SSL_CA_CERT (TRUE / FALSE). +
Used only in combination with SASL_SSL or SSL/TLS client certificate.|FALSE
|KAFKA_SSL_CLIENT_CERT|Path to client certificate file in pem format|
|KAFKA_SSL_CLIENT_CERT_CHAIN|Path to client certificate chain file in pem format|
|KAFKA_SSL_CLIENT_CERT_KEY|Path to client key in pem format|
|KAFKA_SSL_CLIENT_CERT_KEY_PASSWORD|Password for client key|
|KAFKA_TOTAL_BUFFER_SIZE_MB|Memory buffer size for Kafka message buffer in Megabyte. Maximum for the allocated memory for buffered Kafka messages before delivery. +
This amount of memory is per Thread so the maximum overall memory consumption for Kafka buffers is KAFKA_TOTAL_BUFFER_SIZE_MB * INITIAL_WORKER_THREADS. +
If the amount is not sufficient at runtime then the value of KAFKA_MAX_BULK_COUNT is automatically decreased by the application until it is according to the available memory.|100
|LOG_LEVEL|Log level of application (debug, info, warn, error)|info
|MAX_FAILED_LOGONS_BEFORE_ACCOUNT_LOCKED|Number of failed logons to GUI before the used user account will be locked and has to be unlocked by an admin user|3
|MAX_PARTITIONS_TO_COUNT_AS_HEALTHY|If using partitions for table EVENT_LOGS, then this is the max. number of partitions, up to which the system is considered healthy. +
If the number of partitions exceeds this value than a problem is assumed in the transfer to Kafka.|15
|MAX_TRANSACTION_SIZE|Maximum number of messages for processing within one transaction (both DB and Kafka). May be overbooked up to twice the number for special circumstances.|10000
|MAX_SIMULTANEOUS_TABLE_INITIALIZATIONS|Maximum number of simultaneously processed initial transfers of table data after first trigger generation (number of tables)|5
|MAX_SIMULTANEOUS_TRANSACTIONS|Maximum number of transactions simultaneously processing inserts into table EVENT_LOGS without serialization. +
This value controls the setting for INI_TRANS for ORACLE.
Changing this setting requires that there are no pending transactions on table Event_Logs at next startup of the application container.
Otherwise error ORA-00054 is raised and application does not start. +
You should ensure that this value is higher than the expected maximum number of simultaneous transactions on table EVENT_LOGS (User transactions firing triggers + worker threads). +
Reaching this limit with the number of simultaneous pending transactions at one DB block may lead to mismatches in processing order of events for Oracle DB
because SELECT FOR UPDATE SKIP LOCK skips also unlocked records in DB blocks with full ITL (interested transaction list).
|60
|MAX_WORKER_THREAD_SLEEP_TIME|Max. seconds an idle worker thread may sleep until next lookup for events to process|60
|PARTITION_INTERVAL|Interval in seconds between partition changes for table EVENT_LOGS. +
Partition change is used to free already used storage after some seconds and keep the footprint of table EVENT_LOGS as small as possible. +
Relevant only if EVENT_LOGS is used partitioned. +
Changing this setting requires that there are no pending transactions on table Event_Logs at next startup of the application container.
Otherwise error ORA-00054 is raised and application does not start.
|60 seconds
|PUBLIC_PATH|Additional suffix to GUI URL if not the root URL of a host is used (e.g. if locations in nginx are used with URL like https://host/sub_path) +
Ensures that API calls and js/css loads are properly extended with the used sub-path.|''
|RAILS_MAX_THREADS|Maximum number of threads for the underlying Puma application server, should be set to greater than INITIAL_WORKER_THREADS + 30 if default is not sufficient|300
|RUN_CONFIG|Path and name of configuration file in YML format as alternative to configuration by environment variables|APP_ROOT/config/run_config.yml
|SECRET_KEY_BASE|Server side key used for encryption and signing of the JWT that is used for authentication|
|SECRET_KEY_BASE_FILE|Location of file with server side key used for encryption and signing of the JWT that is used for authentication|
|TNS_ADMIN|Directory of config file tnsnames.ora for resolution of Oracle DB aliases (File tnsnames.ora is usually mounted into Docker-Container). Valid for Oracle only.|
|TZ|Sets local timezone within the Docker-container of the applikation. Must be directly set as environment of container during 'docker run' like '-e TZ="Europe/London"', does not work from config file.|Europe/Berlin
|===
==== Configure MOVEX-CDC for connecting to Kafka ====
At least you have to specify the broker hosts and ports to use:
[source]
KAFKA_SEED_BROKER: broker1.mydomain.com:2094,broker2.mydomain.com:2094


MOVEX CDC supports connections to Kafka authenticated by one of the following methods.

===== Using PLAINTEXT authentication for Kafka =====
Nothing needs to be configured in MOVEX_CDC except KAFKA_SEED_BROKER. No authentication takes place.

===== Using SASL_PLAIN authentication to Kafka =====
Username and password are required for connection. Network traffic is not encrypted.
[source]
KAFKA_SASL_PLAIN_USERNAME: kafka_user
KAFKA_SASL_PLAIN_PASSWORD: kafka_password

===== Using SASL_SSL authentication to Kafka =====
Username and password are required for connection. Network traffic is encrypted.
[source]
KAFKA_SASL_PLAIN_USERNAME: kafka_user
KAFKA_SASL_PLAIN_PASSWORD: kafka_password
KAFKA_SSL_CA_CERTS_FROM_SYSTEM: TRUE

or
[source]
KAFKA_SASL_PLAIN_USERNAME: kafka_user
KAFKA_SASL_PLAIN_PASSWORD: kafka_password
KAFKA_SSL_CA_CERT: /.../root-ca.pem, /.../company-ca.pem, /.../issuing-ca.pem

===== Using SSL/TLS authentication to Kafka with client certificate =====
Authentication is based on client certificates.
The required setup of Kafka for SSL is described at http://kafka.apache.org/documentation.html#security_ssl. +
Specifying KAFKA_SSL_CLIENT_CERT_CHAIN is optional in this case.
[source]
KAFKA_SSL_CA_CERT: /.../root-ca.pem
KAFKA_SSL_CLIENT_CERT_CHAIN: /.../ca_chain.pem
KAFKA_SSL_CLIENT_CERT: /.../cert.pem
KAFKA_SSL_CLIENT_CERT_KEY: /.../key.pem
KAFKA_SSL_CLIENT_CERT_KEY_PASSWORD: mykeypw

====== Steps to convert the keystore content generated by Kafka into the formats needed for MOVEX CDC ======
If the client certificates generated for Kafka are in JKS format, the underlying library 'ruby-kafka' of MOVEX CDC does not support this format.
It is possible to convert the generated files into X509 format.
The following steps for conversion are based on this guide: https://github.com/zendesk/ruby-kafka/wiki/Creating-X509-certificates-from-JKS-format.

Preconditions for the next steps are the openssl command line tools "keytool", "openssl"
and optionally the GUI-tool "Keystore Explorer" (https://keystore-explorer.org)

Location and passwords are used as environment variables.

*1. Extract the alias name used in client keystore file*

Identify the second alias name other than 'caroot' and use this alias in next steps for $ALIAS.

[source]
keytool -list -v -keystore $KAFKA_CERT_DIR/kafka.client.keystore.jks -storepass $SSL_KEYSTORE_PASSWORD | grep -i alias

*2. Extract the signed client certificate*

[source]
keytool -noprompt -keystore $KAFKA_CERT_DIR/kafka.client.keystore.jks -exportcert -alias $ALIAS -rfc -storepass $SSL_KEYSTORE_PASSWORD -file client_cert.pem

*3. a. Extract the client key with command line tools*

New client certificate key password becomes the same like source keystore password in this example.

[source]
keytool -noprompt -srckeystore $KAFKA_CERT_DIR/kafka.client.keystore.jks -importkeystore -srcalias $ALIAS -destkeystore cert_and_key.p12 -deststoretype PKCS12 -srcstorepass $SSL_KEYSTORE_PASSWORD -storepass $SSL_KEYSTORE_PASSWORD

[source]
openssl pkcs12 -in cert_and_key.p12 -nocerts -nodes -passin pass:$SSL_KEYSTORE_PASSWORD -out client_cert_key.pem

After generation open the file 'client_cert_key.pem' in an editor and remove all attributes at top so the file content now starts with "-----BEGIN PRIVATE KEY-----".

*3. b. Extract the client key with "Keystore Explorer" as alternative to 3. a.*

* Open file $KAFKA_CERT_DIR/kafka.client.keystore.jks im Keystore Explorer
* Choose the alias identified in step 1
* Choose menu 'Export' / 'Export private key', use format 'openssl'

*4. Extract CA certificate*

[source]
keytool -noprompt -keystore $KAFKA_CERT_DIR/kafka.client.keystore.jks -exportcert -alias CARoot -rfc -file ca_cert.pem -storepass $SSL_KEYSTORE_PASSWORD

Now you are prepared with the four values for KAFKA_SSL_xxx needed to configure SSL connection in MOVEX CDC. +
If KAFKA_SSL_CA_CERT is not set then KAFKA_SSL_CA_CERTS_FROM_SYSTEM should be set to TRUE to rely on system's CA certificate.

=== Running MOVEX Change Data Capture ===
The application is provided as Docker-Image by:
[source]
docker pull ottogroupsolutionproviderosp/movex-cdc

==== Start Docker container ====
You can run the this image like:
[source]
docker run -p 8080:8080 \
  --stop-timeout=120 \
  -e RUN_CONFIG=/etc/run_config.yml \
  -v /my_local_dir/run_config.yml:/etc/run_config.yml \
  ottogroupsolutionproviderosp/movex-cdc

The web-GUI would be available by http://localhost:8080 in this case.
It is recommended to place an own reverse proxy nearby for SSL encryption.

==== Stop Docker container ====
To stop the Docker container you should provide a timeout (at "docker run" or with "docker stop") that allows MOVEX CDC to gracefully shutdown all worker threads before Docker terminates hard with "kill -9".

 docker stop -t 120 <container name/id>

==== Fault tolerance regarding availability of DB and Kafka ====
* At start time of the Docker instance of MOVEX CDC the database must be accessible for connections. +
This is needed to successfully execute the schema initialization once at startup. +
If the DB is not available at Docker instance start or the DB user lacks needed grants, quota etc. then MOVEX CDC terminates with the according error messages in log output.
* Unavailability of Kafka service at Docker instance start can be tolerated.
* Temporary unavailability of DB or Kafka is tolerated by MOVEX CDC without terminating the whole application. +
Health state switches from 200 to http response code 409 in this case, all transfer worker threads are terminated.
Each minute the application tries to successfully restart the expected number of worker threads with their connections to DB and Kafka.
* The event capturing function of the triggers is not influenced by temporary outages of connections or of the whole MOVEX CDC application,
only transfer of events to Kafka is interrupted in this case.

==== Uniqueness of container instances ====
Depending on the database type you may run multiple MOVEX CDC container instances at one database or not.

.Multiple instances allowed for MOVEX CDC
[cols="~,~,~"]
|===
|DB type|Multiple instances with same configuration (same DB schema for MOVEX CDC)|Multiple instances with different configuration (different MOVEX CDC schemas, different Kafka targets)

|SQLite
|Not allowed: No synchronization between multiple instances exist
|Not allowed: No config-specific trigger names are used
|ORACLE
|Possible: Messages to transfer to Kafka are selected with FOR UPDATE.
|Possible: Trigger names contain numeric hash value of MOVEX CDC's owner schema. +
Therefore multiple triggers from several independent MOVEX CDC configurations at one table are possible.
|===

WARNING: But be aware if running multiple container instances of MOVEX CDC on the same database schema (same configuration) simultaneously: +
MOVEX CDC cannot guarantee the exact order of messages with key for transfer to Kafka in this case!


=== Configuration of capturing with GUI ===
TODO: Describe GUI workflow

==== Managing users, schemas and rights ====
Menu "Users" shows the already created named users. Initially there is always a predefined user 'admin'. +
Users are identified by E-Mail.
For authentification at logon one DB-User is associated to each application user of MOVEX CDC, the password of this DB-user is used for logon.

The application user is authorized for certain schemas for which tables can be tagged for event capturing.
This schemas can be picked from the list of schemas where the user has select grants at at least one table of this schema.

==== Configuring events to capture at column level ====
This dialog shows:

* schemas for which the application user has the right to configure (set in user configuration)
* already configured tables of a schema (limited to tables where the user has SELECT grants for)
* columns of a configured table with marks for Insert/Update/Delete-trigger

Possible configuration actions are:

* add tables to configuration for a schema (only possible for tables where the user is allowed to select from)
** modify topic name per table
** choose a value for Kafka key (None / Primary key / Fixed value / Transaction-ID )
** decide if transaction-ID should be recorded in events (adds approx. 0.3 ms per triggering SQL execution)
** decide wether the current content of the table should be initially transferred to Kafka at trigger deployment or nor
* modify triggering of change events per column
** Define the operations (insert/update(delete) to capture for a column
** Define optional filter conditions per operation +
This filter conditions my rely on column values inside the trigger and may also contain subselects to other tables +
Example `:new.Amount > 12 AND 2=(SELECT Company FROM Other_Table WHERE ID=:new.Other_Table_ID)`

NOTE: The configuration in this screen is not user-specific. Each table/column configuration exists only once and can be manipulated by several permitted users.

==== Generation of database triggers ====

=== Troubleshooting ===
==== Common error traps ====
===== Kafka =====
List of Kafka error codes is avaliable here: https://kafka.apache.org/protocol#protocol_error_codes

.possible problems accessing or using Kafka
[cols="~,~,~"]
|===
|Error|Description|Solution

|Kafka::UnknownError: Unknown error with code 53
|TRANSACTIONAL_ID_AUTHORIZATION_FAILED +
The transactional id used by MOVEX CDC is not authorized to produce messages
|Explicite authorization of transactional id is required, optional as wildcard: +
kafka-acls --bootstrap-server localhost:9092 --command-config adminclient-configs.conf
--add --transactional-id * --allow-principal User:* --operation write
|Kafka::UnknownError: Unknown error with code 87
|INVALID_RECORD +
This record has failed the validation on broker and hence will be rejected.
|Possible reason: Log compaction is activated for topic (log.cleanup.policy=compact) but events are created by MOVEX CDC without key. +
Prevent from sending 'tombstone events' without key in this case.
|===

===== Oracle-DB =====
* If TNS alias is used for DB_URL but no tnsnames.ora available at TNS_ADMIN then the JDBC driver treats the TNS alias as host:port:sid with several possible error messages (host does not exist etc.)
* Oracle's number format for values between -1 and 1 is not JSON-compatible (0,123 = .123).
Up to Rel. 12.2 the patch https://support.oracle.com/epmos/faces/PatchResultsNDetails?_adf.ctrl-state=19z17iq454_4&releaseId=600000000018520&requestId=21922926&patchId=27486853&languageId=0&platformId=226&searchdata=%3Ccontext+type%3D%22BASIC%22+search%3D%22%26lt%3BSearch%26gt%3B%26lt%3BFilter+name%3D%26quot%3Bpatch_number%26quot%3B+op%3D%26quot%3Bis%26quot%3B+value%3D%26quot%3B27486853%26quot%3B%2F%26gt%3B%26lt%3B%2FSearch%26gt%3B%22%2F%3E&_afrLoop=164497543848765[27486853] is needed to generate valid JSON in this case.

===== Docker / Kubernetes =====
The Docker container of MOVEX CDC produces a continous log output which can become quite large over time.
You should ensure that logfile size of the Docker container is not unlimited because this may end up in full filesystem. +
For Docker you can configure this behaviour in /etc/docker/daemon.json like this:

[source]
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  }
}

===== Nginx as reverse proxy for SSL encryption or multiple instances =====
If using URL suffixes in nginx locations, then MOVEX CDC container instance has to know this to ensure that all requests to backend API or js and css loads are proper qualified with the used sub-path.

nginx.conf of myhost may look like this:
[source]
http {
  server {
    listen 80 default_server;
    listen [::]:80 default_server;
    server_name _;
    location /mysubpath {
      proxy_pass http://$host:8080/;
    }
  }
}

MOVEX CDC is running at the same host at port 8080. +
The GUI URL is http://myhost/mysubpath in this case.

The MOVEX CDC instance should be started in this case with PUBLIC_PATH="/mysubpath".

=== Logging ===
Logging is done via console output of the Docker container. +
The logging level can be set in startup configuration (LOG_LEVEL) and can be changed dynamically via GUI or API.

=== Monitoring ===
The health state of the Docker container is refreshed every 5 minutes by internally calling the health_check API endpoint of the application.
Additional health information is available by calling:
[source]
http://<MOVEX CDC URL>/health_check

=== Statistics ===
Throughput values of the application are cumulated in the database table "Statistics".
For table, operation and time period several values are recorded.

.throughput parameters recorded in Statistics
[cols="~,~"]
|===
|Column name |Description

|Events_Success|Number of successful processed events
|Events_Delayed_Errors|Number of erroneous single event processings ending in another retry after delay
|Events_Final_Errors|Number of erroneous single event processings ending in final error after retries
|Events_D_and_C_Retries|Number of additional event processings due to divide&conquer retries
|Events_Delayed_Retries|Number of additional event processings due to delayed retries
|===

At first this values are cumulated for each minute. Later on statistics data will be compressed for greater time periods:

* After 14 days values per minute are compressed to values per hour
* After 3 months values per hour are compressed to values per day

Compression is executed once a day as background job in the application.

== Technical implementation ==
=== Structure of Kafka messages ===
MOVEX CDC creates Kafka messages with JSON-formatted content. +
Depending on table configuration Kafka messages may contain an additional key value which drives the assignment of messages to partitions (messages with same key are stored in the same partition).

.Value conversion from database column to JSON value
[cols="~,~,~"]
|===
|JSON representation|Example|Oracle data types

|Number|45.23|BINARY_DOUBLE, BINARY_FLOAT, FLOAT, NUMBER
|String|"Value"|CHAR, CLOB, NCHAR, NCLOB, NVARCHAR2, LONG, ROWID, UROWID, VARCHAR2
|String|"2020-02-21T12:07:43"|DATE
|String|"2020-02-21T12:07:43,396153000"|TIMESTAMP
|String|"2020-02-21T12:07:43,396142000+00:00"|TIMESTAMP WITH TIME ZONE
|String|"90FF"|RAW
|===


.Field names used in Kafka message
[cols="~,~"]
|===
|Fieldname|Explanation

|id|consecutive unique message ID, describes the order of message creation at database trigger level
|schema|schema name of database table
|tablename|name of database table
|operation|kind of triggering database operation (INIT / INSERT / UPDATE / DELETE)
|dbuser|database user who run the triggering operation
|timestamp|detailled timestamp of triggering event
|transaction_id|unique ID of database transaction (optional)
|old|values of observed columns before triggering change event
|new|values of observed columns after triggering event
|===

==== Example of initial load message ====

[source, json]
{
  "id": 23423274179,
  "schema": "EINKAUF",
  "tablename": "HUGO",
  "operation": "INIT",
  "dbuser": "MEYER",
  "timestamp": "2020-02-21T12:07:43,396142+00:00",
  "transaction_id": null,
  "new": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": null
  }
}

==== Example of INSERT message ====

[source, json]
{
  "id": 23423274179,
  "schema": "EINKAUF",
  "tablename": "HUGO",
  "operation": "INSERT",
  "dbuser": "MEYER",
  "timestamp": "2020-02-21T12:07:43,396142+00:00",
  "transaction_id": "9.5.374674",
  "new": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": null
  }
}

==== Example of UPDATE message ====

[source, json]
{
  "id": 234232741379,
  "schema": "EINKAUF",
  "tablename": "HUGO",
  "operation": "UPDATE",
  "dbuser": "MEYER",
  "timestamp": "2020-02-21T12:07:43,396142+00:00",
  "transaction_id": "9.5.374674",
  "old": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": null
  },
  "new": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAACAAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": null
  }
}

==== Example of DELETE message ====
[source, json]
{
  "id": 2342327412279,
  "schema": "EINKAUF",
  "tablename": "HUGO",
  "operation": "DELETE",
  "dbuser": "MEYER",
  "timestamp": "2020-02-21T12:07:43,396142+00:00",
  "transaction_id": null,
  "old": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": null
  }
}


=== Module overview ===
image::module_overview.svg[format=svg,opts=inline]

==== Activities at application startup ====

The following things are executed at startup of application / docker container if necessary:

* The needed data structures in MOVEX CDC's DB schema (defined by DB_USER) are created or updated
* The initial application user "admin" is created for GUI logon with link to the DB_USER for authentication
** For initial GUI logon with user "admin" the password is the DB-passwort of MOVEC CDC's DB-user (DB_PASSWORD)
** The GUI user "admin" acts as supervisor with the authorization to administrate further user accounts

==== Generation of database triggers

===== Creating triggers based on configuration
===== Initial transfer of table content at first trigger generation
If requested in table config, after generation of trigger a job will be created for transfer of the already existing records of a table to Kafka. +
For each record in the table existing at the time of trigger creation an insert-like event will be transferred to Kafka.
The field 'operation' is marked INIT instead of INSERT to be able to distinguish between real insert events and initial load events.
The table's filter condition for insert operation as well as the filter condition for initialization are considered. +
This jobs are queued and processed deferred asynchronously.
The maximum number of simultaneously processed table initialization jobs is limited by the environment setting MAX_SIMULTANEOUS_TABLE_INITIALIZATIONS.

Precondition for initial transfer of table data is that MOVEX CDC's DB user is allowed to read this table by SELECT, because initial transfer is done directly by selecting from table, not by trigger execution.

.Techniques used to determine rows for initial transfer
[cols="~,~"]
|===
|Database|Technique

|Oracle|Flashback query by SCN can be used to select from the table in it's state directly after insert trigger check/creation.

To be 100% sure that each record is transferred either by initial transfer or by trigger event the following conditions must be valid: +
- There should not be pending transactions for this table at the time of trigger creation because this uncommited records are not catched later by "SELECT ... AS OF SCN" +
- There should not be insert operations during the trigger creation because this may result in duplicate insert events from initialization and trigger +
- The SCN targets to the generation timestamp of the initial load job (directly after trigger creation if triggers have to be deployed)

|===


==== Transfer of triggered events to Kafka ====
An consecutive ID is used to define the order of message creation at trigger level. +
This ID allows the reconstruction of the original order of messages in Kafka even if using topics with multiple partitions.

Event transfer to Kafka is done by MOVEX CDC with multiple concurrent threads. +
Each transfer thread has it's own connection to source database as well as to Kafka. +
To guarantee the original creation order of events also during transfer to Kafka,
exactly one of MOVEX CDC's transfer threads is responsible for transfer of all events with the same key. +
That means, events without a key can be transferred by every thread, events with a key are transferred by one particular thread determined by a hash value of the key and a modulo operation.

===== Scaling transfer capacity / troughput between MOVEX CDC and Kafka =====
Scalability is given by configurable number of worker threads (INITIAL_WORKER_THREADS) in the MOVEX CDC application, each working isolated with own DB and Kafka session. +
Depending on the capacity of the runtime env. (DB, CPU, network, Kafka) several 100 worker threads are possible.

Example throughput with Oracle DB can be up to 300,000 events per minute and worker thread
if message size is below the magic 4K (no content storage in CLOB).

===== Error handling during transfer =====
Transferring is done with bulk operations against database and Kafka.
If the transfer operation fails the bulk size would be reduced (divide & conquer) until a single event is processed in it's own transaction. +
If this single processing still fails then the event is marked in Table Event_Logs and suspended for processing for the time defined by ERROR_RETRY_START_DELAY.
After a number of not successful retries (defined by ERROR_MAX_RETRIES) the erroneous event is moved to table 'Event_Log_Final_Errors'.

Events moved to final error table can be rescheduled by API function: _TODO: mark API function_

If no further action happens then this event is erased from table 'Event_Log_Final_Errors' by a houskeeping process after FINAL_ERRORS_KEEP_HOURS.

Reasons for transfer errors can be for example:

* non-existing Kafka topic
* exceeding the maximum event size for Kafka topic
* event without key but log compaction set for Kafka topic

===== Implementation details of transfer for Oracle =====
SELECT FOR UPDATE SKIP LOCKED is used to isolate the concurrent worker threads so an event can be prcessed by one thread only. +
The value of MAX_SIMULTANEOUS_TRANSACTIONS (default 60) controls the INI_TRANS setting for table EVENT_LOGS.
This value defines the maximum number of concurrent transactions (trigger + worker threads) that are supported at a particular DB block before serialization takes place.
Serialization also influences the correct event sequence at SELECT FOR UPDATE SKIP LOCKED,
so please ensure to set this value higher than the expected maximum number of simultaneous DB transactions on table Event_Logs.

====== Oracle Enterprise Edition with Partitioning Option ======
The staging table Event_Logs uses interval partitioning with an default interval of 60 seconds.
You can control this interval by PARTITION_INTERVAL. +
The partitioned table Event_Logs does not have any index,
this way eliminating a remaining risk of blocking locks during index block split operations at inserts executed by trigger. +
The limited size of a single partition allows to read a partition by full table scan with predictable effort. +
After beeing completely transferred to Kafka, empty partitions are deleted by the housekeeping job.
So the total size of the table descreases after temporary burst loads in contrast to the high water mark of a common heap table.

====== Oracle Standard Edition or EE without Partitioning Option ======
For Oracle Standard Edition rsp. Enterprise Edition without Partitioning Option the staging table EVENT_LOGS is implemented as a regular heap table with an index on column ID.
That means: several optimizations based on partitioning do not take place.

* The staging table EVENT_LOGS needs an index on column ID for proper performance.
This adds additional index maintenance load on triggering transaction and a very tiny risk of blocking between concurrent transactions at index block split operations.
* The high water mark of table EVENT_LOGS is not automatically reduced after peak usage.
* Additional reorganization activities on staging table EVENT_LOGS can by necessary from time to time depending on type and frequency of usage:
** ALTER TABLE Event_Logs MOVE; to reduce the high water mark
** ALTER INDEX Event_Logs_PK REBUILD; to reduce the size of the index

==== Houskeeping ====

==== Health check ====
The healthcheck service is available at:

 http://<MOVEX CDC URL>/health_check

It can be called maximum once a second.
The http-response contains a JSON-object with detailled informations.
There's no authentification needed for execution of health check.
The response status code contains the health status of the running instance:

- 200 (ok): Health Check o.k., the configured number of worker threads exists and is functional.
- 409 (conflict): Health check recognized a problem in operation
- 500 (internal server error): Technical problem during processing of health check request or called too frequently (further details in response body)

==== Export and import of configuration data ====
The content of configuration tables can be exported as a consistent JSON document.
This JSON document can also be imported to a MOVEX-CDC instance. +
This way the configuration data can be stored as a backup outside the database and MOVEX-CDC instance.
Import and export requires authentication as user with admin rights.
This function is available in MOVEX-CDC's GUI at menu 'Administration/Config exchange' and also as raw http API.

===== Export of configuration data =====
The export to a JSON document can be executed for a particular DB schema or for all schemas.
It exports schemas, tables, columns, conditions, schema rights and users.

This example exports the whole configuration data into a file +
[source]
curl -X GET -H "Authorization: \
`curl -d "email=admin&password=<my_password>" http://localhost:8080/login/do_logon | \
jq .token | sed -e 's/^"//' -e 's/"$//'`" \
http://localhost:8080/import_export/export > movex-cdc-config.json

With schema name in parameter 'schema' the exported is limited to a particular schema. +
Data of all configured users is included in the JSON document in both cases.

===== Import of schema configuration data =====
The import from JSON data can be done for the whole content or you may pick only one schema to import. +

This example imports a particular schema from a JSON document with several schemas:
[source]
curl -X POST -H "Authorization: \
`curl -d "email=admin&password=<my_password>" http://localhost:8080/login/do_logon | \
jq .token | sed -e 's/^"//' -e 's/"$//'`" \
-d "json_data=`cat movex-cdc-config.json | sed 's/"/\\"/g'`" \
http://localhost:8080/import_export/import

===== Import of user configuration data =====
If importing one or all schemas from JSON file only users with rights for this schemas are created if they don't yet exist in DB.
Missing users are created with locked account in this case to avoid unwanted security issues.

To import all users with all their attributes from JSON document there's a separate function.


=== API endpoints ===
Most of the API endpoints are useful only when called from GUI, but several of this API endpoints may also be useful for calling from outside the application. +
API Responses are JSON objects.

.API endpoints for additional usage from outside the application
[cols="~,~,~,~,~"]
|===
|Verb|URL|Parameter|Response|Description

|POST|/db_triggers/generate|schema_name: limit deployment to this schema+
dry_run=true: optional (default=false), checks only for differences without executing DDL +
table_id_list: optional (default=all), array with IDs from config table 'TABLES', only for this tables triggers will be deployed|JSON object with successful generated triggers and errors|Check for difference between existing triggers and current configuration, generate and execute the needed DDL statements. Executes for all schemas where the login user has the deployment grant.
|POST|/db_triggers/generate_all|dry_run=true: optional (default=false), checks only for differences without executing DDL +
table_id_list: optional (default=all), array with IDs from config table 'TABLES', only for this tables triggers will be deployed|JSON object with successful generated triggers and errors|Check for difference between existing triggers and current configuration, generate and execute the needed DDL statements. Executes for all schemas where the login user has the deployment grant.
|GET|/health_check|no|JSON object with several application status info|ask health status (200=ok) and get some condensed status information
|GET|/health_check/log_file|no|current log file of application|Download log file of MOVEX CDC application. +
Requires valid user JWT in request header.
|GET|/import_export/export|schema (Limit export to a single schema, optional)|JSON object|Export configuration data of all or a particular schema (users, schemas, tables, columns, conditions, schema rights, users) as JSON object
|POST|/import_export/import|JSON object, schema (Limit exports to a single schema, optional)|no|Import configuration data for users and schemas. Each user / schema contained in JSON object creates/replaces the configuration data in the applications config tables
|POST|/import_export/import_all_users|JSON object|no|Import the complete configuration data for users from JSON object. The schema import in contrast imports only the users that are necessary for dependencies with locked account.
|POST|/login/do_logon|email, password|token|Validate user authentication, get JWT token for authentication/authorization of following requests
|POST|/server_control/reprocess_final_errors|schema_name, table_name (optional)|reprocess_count: Number of rescheduled events|Move stored erroneous events from table Event_Log_Final_Errors to table Event_Logs for repeated transfer to Kafka
|POST|/server_control/set_log_level|log_level (DEBUG, INFO, WARN, ERROR, FATAL)|no|Set log level of server instance, requires valid admin JWT in request header
|POST|/server_control/set_max_transaction_size|max_transaction_size (1..infinity)|no|Set the number of events to process within on DB and Kafka transaction, requires valid admin JWT in request header
|POST|/server_control/set_worker_threads_count|worker_threads_count (0..200)|no|Set number of active worker threads, requires valid admin JWT in request header
|POST|/server_control/terminate|no|no|Graceful shut down the current container instance of MOVEX CDC by sending SIGTERM to the application, requires valid admin JWT in request header
|===

==== Using API endpoints from command line ====
You can use curl or wget to call API funktions with valid autorization by email and password. +
Steps are:

* authenticate with valid user and get JWT token for next steps
* call API methode with use of JWT

To use this examples replace the values for email, password, host and port with yours. +
Needed tools are curl, jq, sed. +

===== API example for setting log level to DEBUG =====
[source]
curl -X POST -H "Authorization: \
`curl -d "email=admin&password=<my_password>" http://localhost:8080/login/do_logon | \
jq .token | sed -e 's/^"//' -e 's/"$//'`" \
-d "log_level=ERROR" \
http://localhost:8080/server_control/set_log_level

===== API example for trigger deployment without executing DDL =====
```
JWT=`curl -s -d "email=admin&password=<my_password>" http://localhost:8080/login/do_logon | \
jq .token | \
sed -e 's/^"//' -e 's/"$//'`

curl -s -X POST -H "Authorization: $JWT" -d "dry_run=true" http://localhost:8080/db_triggers/generate_all
```

=== Entity relationship model ===
image::er_model.svg[format=svg,opts=inline]

== Security considerations ==
=== Encryption of network traffic ===
==== Encrpytion of DB connection ====
Connections to Oracle-DB are unencrypted until now.
Encryption of DB connection will be default soon after release of https://github.com/rsim/oracle-enhanced/pull/2284[this pull request] for the underlying DB adapter. +
Precondition for DB network encryption is that the DB server ist configured in sqlnet.ora with at least "SQLNET.ENCRYPTION_SERVER = ACCEPTED".

==== Encrpytion of Kafka connection ====
CAUTION: TODO: Describe preconditions for encrypted traffic between MOVEX CDC and Kafka

==== SSL encryption for https access on GUI ====
There is no SSL/HTTPS encryption for the GUI of MOVEX CDC out of the box. +
To ensure encypted HTTP traffic you should place MOVEX CDC behind a reverse proxy or ingres controller with SSL encryption. +

CAUTION: TODO: Example config for MOVEX CDC behind nginx with docker compose should be added

=== Access authentication for GUI ===
* Users authenticate at logon with the password of the corresponding DB user
* A JWT token is created at GUI logon and used for subsequent API calls
* This JWT token is signed by a key that is stored in the local file config/secrets.yml.
This key can be defined by several ways:
** The key is generated at first startup if neither SECRET_KEY_BASE nor SECRET_KEY_BASE_FILE is given
** The key is given by environment variable SECRET_KEY_BASE
** The key ist given in a file pointed to by environment variable SECRET_KEY_BASE_FILE
* Usually the generated key should be sufficient. This key changes only at recreation of Docker container.

=== Lock user account after multiple failed logons ===
User account is locked after 3 subsequent failed logon tries. +
Unlocking a locked account is possible via GUI for admin users.

=== Suppress frequent access ===
* Email/password check at /login/do_logon is delayed for up to 5 seconds if subsequent logon requests occur within 5 seconds
* Subsequent calls to /health_check are rejected within the same second

=== Isolation between the production schemas of the DB and MOVEX CDC's schema ===
MOVEX CDC requires an own schema at the database. This schema must not contain any foreign structures.
All database changes made by MOVEX CDC are isolated to this schema (including te generated triggers).
The owner of MOVEX CDC's DB schema requires only a minimum set of rights on foreign objects, especially no right to read the full table content (except if initialization is requested).

=== Restricted definition of triggers ===
There might be a security gap if users may define trigger on tables where they don't have read rights.
This way they could possibly read hidden table content via Kafka. +
Therefore only tables are accessible for trigger definition in the GUI where the DB user associated with the application user has at least read rights.

== Frequently asked questions ==
.FAQ will be completed over time with all upcoming questions that are not answered befor
[cols="~,~"]
|===
|Question|Answer

|Do I have to redeploy triggers if I want to change the topic of a table|
The target topic for a table is read from current configuration before transferring events to Kafka. +
Therefore there is no need to redeploy the triggers in this case. +
Each worker thread caches the topics of a table or schema for max. 60 seconds before refreshing it with the current configuation.
If want to exactly define the timestamp of change, then: +
- Set the worker count to 0 +
- Wait until all worker threads have finished. You can check this by health check. +
- change the topic in GUI +
- set the worker count to > 0. Now the new topic is used for transfer to Kafka.

|===
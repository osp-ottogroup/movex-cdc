= image:osp.png[float="left" width=200 ] MOVEX Change Data Capture: Quick start demo =
Author: Peter Ramm ( Peter.Ramm@og1o.de )
:Author Initials: PR
:toc: preamble
:toclevels: 4
:icons:
:imagesdir: ./images
:numbered:
:sectnumlevels: 6
:homepage: https://www.osp.de
:title-logo-image: osp.png
:description: Oracle change data capture to Kafka: How to run within 10 minutes
:keywords: Oracle, Kafka, Change Data Capture, CDC, Trigger

**Implement change data tracking on an existing Oracle DB including event transfer to Kafka within 10 minutes
**

== Introduction ==

This demo describes how to use the product *MOVEX Change Data Capture* which is hosted at https://gitlab.com/osp-silver/oss/movex-cdc. +
A detailed documentation of this product you find at: https://osp-silver.gitlab.io/oss/movex-cdc/movex-cdc.html.

== Preconditions
- A running local Oracle database with an existing user to observe

== Prepare environment in OS
Please adjust the values according to your setup.
----
# Physical IP address of the Docker host running Kafka and MOVEX Change Data Capture
# Use real physical IP-addresses for Kafka configuration, not 'localhost'
# Because Kafka uses this addresses also from inside the container where 'localhost' has a different content
export IP_ADDRESS=192.168.178.34

# URL of existing ORACLE DB
export DB_URL=$IP_ADDRESS:1521/ORCLPDB1

# Schema owner and password for MOVEX' internal data structures
export MOVEX_DB_USER=movex
export MOVEX_DB_PASSWORD=movex

# Change if SYS cannot be used for MOVEX schema owner creation( like 'admin' for autonomous DB)
export DB_SYS_USER=sys
# SYS password of Oracle instance if MOVEX schema owner should be created by MOVEX itself
export DB_SYS_PASSWORD=oracle

# User and password of the schema to observe
export SOURCE_DB_USER=my_observed_schema
export SOURCE_DB_PASSWORD=my_observed_schema_pw
----



== Prepare Kafka

=== Run Kafka container
For demo purposes we use an own Docker image that combines Zookeeper and Kafka in one image.
The command will log the Kafka console output and block until termination by Ctrl-C.
----
docker run --rm \
  --name kafka \
  -p 2181:2181 \
  -p 9092:9092 \
  -e IP_ADDRESS=$IP_ADDRESS \
  -e KAFKA_ADVERTISED_LISTENERS=LISTENER_EXT://$IP_ADDRESS:9092,LISTENER_INT://localhost:9093 \
  registry.gitlab.com/osp-silver/oss/movex-cdc/kafka-compact:3.0.0
----

=== Create the Kafka topic
Run the command within the just started Kafka Docker container ("docker exec -ti kafka bash"):
----
$KAFKA_HOME/bin/kafka-topics.sh --create --topic hugo --partitions 4 --bootstrap-server $IP_ADDRESS:9092 --replication-factor 1
----

=== Create a consumer process for the topic
Run the command within in the just started Kafka Docker container ("docker exec -ti <container-ID> bash").
The command will log each new message of the topic and block until termination by Ctrl-C.
----
$KAFKA_HOME/bin/kafka-console-consumer.sh --topic=hugo --bootstrap-server=$IP_ADDRESS:9092 --isolation-level=read_committed
----

== Prepare the MOVEX Change Data Capture application

=== Create config file run_config.yml
----
cat <<EOF >run_config.yml
################################

# Log level for application (debug, info, warn, error)
LOG_LEVEL: debug

# Type of used database (SQLITE, ORACLE)
DB_TYPE: ORACLE

# Username of MOVEX CDC's schema in database
DB_USER: $MOVEX_DB_USER

# Password of DB_USER, also used as password of user 'admin' for GUI logon.
DB_PASSWORD: $MOVEX_DB_PASSWORD

# Database-URL for JDBC Connect: Example for Oracle: "MY_TNS_ALIAS" or "machine:port/service"
DB_URL: $DB_URL

# Comma separated list of seed brokers for Kafka logon
KAFKA_SEED_BROKER: $IP_ADDRESS:9092

################################
EOF
----

=== Create DB-User for MOVEX CDC
If you don't already have a user for MOVEX CDC with the needed privileges at your DB,
then this task ensures that the user exists and has the needed privileges. +
You can run this task to ensure the needed privileges even if the user already exists. +
Precondition for this task is the SYS password provided via DB_SYS_PASSWORD.
----
docker run --rm \
  -e RUN_CONFIG=/etc/run_config.yml \
  -e DB_SYS_PASSWORD=$DB_SYS_PASSWORD \
  -v $PWD/run_config.yml:/etc/run_config.yml \
  ottogroupsolutionproviderosp/movex-cdc bundle exec rake ci_preparation:create_user
----

=== Run MOVEX CDC Docker container
----
docker run --rm \
  --name cdc \
  -e RUN_CONFIG=/etc/run_config.yml \
  -v $PWD/run_config.yml:/etc/run_config.yml \
  -p8080:8080 \
  ottogroupsolutionproviderosp/movex-cdc
----

== Prepare test case

=== Create a table to observe for test user
----
echo "
-- Remove possibly existing objects
BEGIN
  FOR Rec IN (SELECT 1 FROM User_Tables WHERE Table_Name = 'HUGO') LOOP
    EXECUTE IMMEDIATE 'DROP TABLE HUGO';
  END LOOP;
  FOR Rec IN (SELECT 1 FROM User_Sequences WHERE Sequence_Name = 'HUGO_SEQ') LOOP
    EXECUTE IMMEDIATE 'DROP SEQUENCE HUGO_SEQ';
  END LOOP;
END;
/

CREATE TABLE Hugo (
       ID          NUMBER PRIMARY KEY,
       Name        VARCHAR2(30),
       Start_Date  DATE);
CREATE SEQUENCE Hugo_Seq;
GRANT SELECT ON Hugo TO $MOVEX_DB_USER;
GRANT FLASHBACK ON Hugo TO $MOVEX_DB_USER;
" | sqlplus $SOURCE_DB_USER/$SOURCE_DB_PASSWORD@$DB_URL
----

=== Generate permanent changes on the table to observe
----
echo "
  BEGIN
    LOOP
      INSERT INTO Hugo (ID, Name, Start_Date) VALUES (Hugo_Seq.NextVal, 'Name '||Hugo_Seq.Currval, SYSDATE);
      COMMIT;
      DBMS_SESSION.SLEEP(1);
    END LOOP;
  END;
/
" | sqlplus $SOURCE_DB_USER/$SOURCE_DB_PASSWORD@$DB_URL
----


== Start capture in MOVEX CDC's GUI
Open the application in browser: `http://localhost:8080` and login with the predefined user "admin" and the passwort of the MOVEX DB user.

image:login_admin.png[format=png, width=300]

Create your own personal application user: click "Create User"

image:users_initial.png[format=png, width=800]

In the "Create User" dialog:

* Add name and email,
* Choose an existing DB-user for authentication with it's password. +
This can be every DB user including the MOVEX CDC schema owner.
* Check "Admin User" to allow this user administrative tasks
* Click "Create" to establish the user
* Open the user again to add schema rights now
* Add authorized schemas where this user is enabled to configure change tracking
  ** Select a schema from the list of schemas
  ** Check "Deployment granted" to allow creation of triggers for this user
  ** Click "Add" to add this schema to the list of enabled schemas
* Click "Save"

image:create_user.png[format=png, width=800]

Logout as 'admin'

image:logout.png[format=png, width=800]

Reconnect with the just created personal user using email and the password of the associated Oracle user. +
Then choose the menu "Configuration", select the schema to observe and click "Add table to observe".

image:config_select_schema.png[format=png, width=700]

Select a table from the list, then:

 * Set the name of the previously created Kafka topic 'hugo'
 * Decide wether to include the Oracle transaction ID into the event or not
 * Choose the kind of message key handling https://osp-silver.gitlab.io/oss/movex-cdc/movex-cdc.html#_using_kafka_keys_to_ensure_sequential_order_of_messages[(See documentation for details)]
 * Choose wether to transfer the current content of the table into the Kafka topic before tracking further changes or not
 ** Decide if flashback query should by used to initialize with existing records only up to the SCN of trigger creation
 ** Optionally place a filter condition to the initialization
 ** Optionally define the order of records for initialization

image:add_table.png[format=png, width=600]

Now tap on the table to mark it as current, then configuration of columns appear.
Check the columns you want to transfer to Kafka for the particular operation.

image:config_columns.png[format=png, width=800]

If you want to add filter conditions to the three operations,
then click at the filter icon for the operation and add the filter condition. +
Be aware that these conditions are executed within a trigger, so refer to columns of the table by qualifier ":new.column" or ":old.column".

image:add_filter.png[format=png, width=800]

Now all of configuration is done and the tracking can be activated. +
Head over to menu "Deployment", select one or all schemas and click "Generate for schema". +
At first only a dry run of trigger creation is executed.
All tables with differences between configured and active triggers are shown. +
By clicking the rightmost triangle you may list the new trigger syntax according to the configuration as well as the optional initialization code.

image:deploy_dry.png[format=png, width=800]

Check the "Deploy" switch for the tables you want to deploy and hit "Deploy"

image:deploy_trigger.png[format=png, width=800]

Now the triggers are activated in the DB  . If requested the inialization tasks are starting in background. +
After no more than one minute the MOVEX Change Data Capture will stop sleeping idle and recognize the existence of events to transfer to Kafka. +
You can evaluate the incoming events in Kafka at your already waiting consumer session.

That's it, enjoy the success (hopefully).

The full story and documentation of MOVEX Change Data Capture you may find here: +
https://osp-silver.gitlab.io/oss/movex-cdc/movex-cdc.html

== Add ksqlDB
This is an additional task that's not really necessary for this showcase itself. +
If you want it shows how to handle the event stream using SQL-like syntax.

=== create docker-compose.yml
----
cat <<EOF >docker-compose.yml
---
version: '2'

services:
  ksqldb-server:
    image: confluentinc/ksqldb-server:0.11.0
    hostname: ksqldb-server
    container_name: ksqldb-server
    ports:
      - "8088:8088"
    environment:
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: $IP_ADDRESS:9092
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"

  ksqldb-cli:
    image: confluentinc/ksqldb-cli:0.11.0
    container_name: ksqldb-cli
    depends_on:
      - ksqldb-server
    entrypoint: /bin/sh
    tty: true
EOF
----

=== Start ksqlDB
----
docker-compose up
----

=== Connect to ksqlDB CLI
----
docker exec -it ksqldb-cli ksql http://ksqldb-server:8088
----

=== Create stream from topic in ksqlDB CLI
----
CREATE STREAM hugo_stream (msg_key VARCHAR KEY,
                           id INTEGER,
                           schema VARCHAR,
                           tablename VARCHAR,
                           operation VARCHAR,
                           timestamp VARCHAR,
                           new STRUCT<NAME VARCHAR, ID INTEGER, START_DATE VARCHAR>)
  WITH (kafka_topic='hugo', value_format='JSON');
----

=== Select from stream in ksqlDB CLI
----
SELECT id, schema, tablename, operation, timestamp, new->NAME,
  new->ID, new->Start_Date FROM hugo_stream EMIT CHANGES;
----


